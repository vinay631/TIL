## Stochastic Gradient Descent
#### Date: Feb 12, 2016

Stochastic Gradient Descent is an optimization algorithm where you use one or few samples from training set to minimize the error function. In SGD, an approximation of the direction that decreases the objective function is calculated. It converges faster and is computationally less intensive than Gradient Descent.

--
