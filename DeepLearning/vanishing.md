## Vanishing Gradient Problem
### Date: Feb 20, 2016

In deep neural network with sigmoid activation units, gradient based learning and backpropagation, the layers in front receive very weak gradients as the gradient decreases exponentially because of chain rule. The updates as we move to the front of the network will contain less information. It can be solved by using Long Short Term Memory architecture (?).

--
